{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run, Workspace, Datastore, Experiment\n",
    "from azureml.core import Dataset as DSET\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import  AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "\n",
    "#seed everything\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "experiment = Experiment(ws, \"oneshot_feedback\")\n",
    "run = experiment.start_logging()\n",
    "\n",
    "#read data using azure dataset (create if it doesn't exist)\n",
    "try:\n",
    "    dset = DSET.get_by_name(ws, \"train\")\n",
    "except: \n",
    "    try:\n",
    "        dstore = DSET.get(ws, \"fdbacktrain\")\n",
    "        dset = DSET.Tabular.from_delimited_files(path = (dstore, \"train.csv\"), support_multi_line=True)\n",
    "        dset.register(ws, \"train\")\n",
    "        print(\"dataset created!\")\n",
    "    except:\n",
    "        dstore = Datastore.register_azure_blob_container(ws, \"fdbacktrain\", \n",
    "                                            container_name=\"blobdata\",\n",
    "                                            account_name=\"fdback3\",\n",
    "                                            account_key=\"RKpT935tVL5g0wHcnlS1cqnynMc1c6iyyvSuifJU+AdoV8UQX*3lCMAVsxwYkp7bOetbbdP*uXa8+AStP8VDvA==\")\n",
    "        dset = DSET.Tabular.from_delimited_files(path = (dstore, \"train.csv\"), support_multi_line=True)\n",
    "        dset.register(ws, \"train\")\n",
    "        print(\"datastore created!\\ndataset created!\")\n",
    "\n",
    "df = dset.to_pandas_dataframe()\n",
    "\n",
    "\n",
    "#********************************************\n",
    "#                Configuration\n",
    "#********************************************\n",
    "class CONF:\n",
    "    model_name = [\"microsoft/deberta-v3-base\",\"microsoft/deberta-v3-large\"]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n_classes = 6\n",
    "    max_len = 512,1024\n",
    "    inter_size= [512, 264, 128, 64]\n",
    "\n",
    "\n",
    "#tokenize text\n",
    "print(\"tokenization step...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONF.model_name[0])\n",
    "def tokenize_func(text):\n",
    "    return tokenizer(text,padding=\"max_length\",truncation=True,max_length= CONF.max_len[0],return_tensors=\"pt\",return_token_type_ids=False)\n",
    "\n",
    "df.full_text = df.full_text.apply(lambda x:x.replace(\"\\n\\n\", \"|\"))\n",
    "df[\"tokens\"] = df.full_text.apply(tokenize_func)\n",
    "\n",
    "    \n",
    "\n",
    "#********************************************\n",
    "#                dataset\n",
    "#********************************************\n",
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, data, istrain):\n",
    "        self.x = data.tokens.values\n",
    "        self.y = data.loc[:,\"cohesion\":\"conventions\"].values\n",
    "        self.istrain = istrain\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.istrain:\n",
    "            return self.x[idx], self.y[idx]\n",
    "        return self.x[idx]\n",
    "    \n",
    "\n",
    "#********************************************\n",
    "#                dataloader\n",
    "#********************************************\n",
    "dtset = custom_dataset(df.iloc[:10], True)\n",
    "train, test, val = random_split(dtset,[6,2,2])\n",
    "# train, test, val = random_split(dtset,[3500,211,200]) #percent only available in v1.13\n",
    "trainloader = DataLoader(train, batch_size=1, shuffle=True, num_workers=0)\n",
    "testloader = DataLoader(test, batch_size=1, shuffle=False, num_workers=0)\n",
    "valloader = DataLoader(val, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "#********************************************\n",
    "#                Model\n",
    "#********************************************\n",
    "base_model = AutoModel.from_pretrained(CONF.model_name[0])\n",
    "hidden_size = base_model.config.hidden_size\n",
    "\n",
    "\n",
    "class MeanMaxPool(nn.Module):\n",
    "    def __init__(self, size_in):\n",
    "        super().__init__()\n",
    "        self.size_in = size_in\n",
    "\n",
    "    def forward(self,x, mask):\n",
    "        CLS = x[:,0,:]        #retreive first vector of CLS\n",
    "        EMBED = x[:,1:,:]         # only words\n",
    "        mask = mask[:,1:].unsqueeze(2).expand(-1,-1, self.size_in).clone()\n",
    "        # mask padding tokens\n",
    "        EMBED = EMBED*mask.clone()\n",
    "        # replace 0 of mask by tiny values to capture the next division\n",
    "        mask[mask==0] = 1e-4\n",
    "        meanpool= ((EMBED.sum(dim=1))/mask.sum(dim=1))\n",
    "        # replace 0 vectors by  huge negative values to never be selected as max\n",
    "        EMBED[mask==1e-4]=-1e9\n",
    "        maxpool= EMBED.max(dim=1).values\n",
    "        mean_max_pool=torch.concat((meanpool, maxpool,CLS),dim=1)\n",
    "        return mean_max_pool\n",
    "\n",
    "\n",
    "class custom_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.mean_max_pool = MeanMaxPool(hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(hidden_size*3,CONF.inter_size[0])\n",
    "        self.out = nn.Linear(CONF.inter_size[0],CONF.n_classes)\n",
    "\n",
    "    def forward(self, ids,mask):\n",
    "        x = self.base(ids, mask)[0]\n",
    "        x = self.mean_max_pool(x,mask)\n",
    "        x= self.fc(x)\n",
    "        x=self.dropout(x)\n",
    "        out=self.out(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "#********************************************\n",
    "#                Training func\n",
    "#********************************************\n",
    "\n",
    "#loss functions\n",
    "def MCRMSE(yhat, ytrue):\n",
    "    return ((yhat-ytrue)**2).mean(dim=0).sqrt().mean()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "def train_one_epoch():\n",
    "    tot_mse = 0\n",
    "    tot_mcr=0\n",
    "    for xbatch, ybatch in trainloader:\n",
    "        model.train()\n",
    "        ids = xbatch.input_ids.squeeze(1).to(CONF.device)\n",
    "        att_mask = xbatch.attention_mask.squeeze(1).to(CONF.device)\n",
    "        ybatch = ybatch.to(CONF.device)\n",
    "        y_hat = model(ids, att_mask)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        mseloss = mse(y_hat.float(), ybatch.float())\n",
    "        tot_mse+=mseloss.item()\n",
    "\n",
    "        mcrloss = MCRMSE(y_hat.float(), ybatch.float())\n",
    "        tot_mcr+=mcrloss.item()\n",
    "\n",
    "        mcrloss.backward()   # reduce mcrmse loss\n",
    "        opt.step()\n",
    "    return tot_mse/len(trainloader), tot_mcr/len(trainloader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_one_epoch():\n",
    "    tot_mse = 0\n",
    "    tot_mcr=0\n",
    "    for xbatch, ybatch in valloader:\n",
    "        model.eval()\n",
    "        ids = xbatch.input_ids.squeeze(1).to(CONF.device)\n",
    "        att_mask = xbatch.attention_mask.squeeze(1).to(CONF.device)\n",
    "        ybatch = ybatch.to(CONF.device)\n",
    "        y_hat = model(ids, att_mask)\n",
    "\n",
    "        mseloss = mse(y_hat.float(), ybatch.float())\n",
    "        tot_mse+=mseloss.item()\n",
    "\n",
    "        mcrloss = MCRMSE(y_hat.float(), ybatch.float())\n",
    "        tot_mcr+=mcrloss.item()\n",
    "\n",
    "    return tot_mse/len(valloader), tot_mcr/len(valloader)\n",
    "\n",
    "\n",
    "def Early_stopping(arr, patience):\n",
    "    gt = arr[1:]>arr[:-1]\n",
    "    if sum(gt[-patience:])==patience:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "#********************************************\n",
    "#                Training\n",
    "#********************************************\n",
    "model = custom_model()\n",
    "model.to(CONF.device)\n",
    "model= nn.DataParallel(model)\n",
    "opt = torch.optim.Adam(model.parameters(), lr= 1e-5)\n",
    "epochs = 2\n",
    "\n",
    "train_mcr = []\n",
    "train_mse = []\n",
    "val_mcr = []\n",
    "val_mse=[]\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "print(\"Training...\")\n",
    "start = time.perf_counter()\n",
    "for epoch in range(epochs):\n",
    "    # gc.collect()\n",
    "    tmse, tmcr = train_one_epoch()\n",
    "    vmse, vmcr = val_one_epoch()\n",
    "    run.log(\"train mse\",tmse)\n",
    "    run.log(\"val mse\",vmse)\n",
    "    run.log(\"train mcr\",tmcr)\n",
    "    run.log(\"val mcr\",vmcr)\n",
    "\n",
    "    train_mcr.append(tmcr)\n",
    "    val_mcr.append(vmcr)\n",
    "    \n",
    "    train_mse.append(tmse)\n",
    "    val_mse.append(vmse)\n",
    "\n",
    "    print(f'epoch {epoch+1}/{epochs}:  train mcrmse: {tmcr: .3f}  ===================== val mcrmse: {vmcr: .3f}')\n",
    "    print(f'epoch {epoch+1}/{epochs}:  train mse:    {tmse: .3f}  ===================== val mse:    {vmse: .3f}\\n')\n",
    "\n",
    "    if val_mcr[epoch]==np.min(val_mcr):\n",
    "        best_wt = model.state_dict()\n",
    "        best_mcr = val_mcr[epoch]\n",
    "        best_ep = epoch+1\n",
    "\n",
    "    early_stop = Early_stopping(np.array(val_mcr),3)\n",
    "    if early_stop:\n",
    "        break\n",
    "\n",
    "finish = time.perf_counter()\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(best_wt, f'models/model_wght_{best_mcr: .3f}_ep_{best_ep}.pt')\n",
    "print(f\"the task took {np.round(finish-start, 0)} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run.register_model(model_name = \"model1\", model_path=\"./models\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.complete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdkv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f986d9203031bfcdc2eae0da509c193e8f45c169e21bfd48a3daebb74003503"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
